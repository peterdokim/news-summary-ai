import os
import requests
import urllib.parse
import numpy as np
from bs4 import BeautifulSoup
from typing import List, Dict, Tuple
from openai import OpenAI
from dotenv import load_dotenv
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_distances

load_dotenv()

class NewsSummarizer:
    def __init__(self):
        self.headers = {
            "User-Agent": (
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/143.0.0.0 Safari/537.36"
            )
        }

        api_key = os.getenv("OPENAI_API_KEY")

        if not api_key:
            raise ValueError(
                "OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                "1. .env íŒŒì¼ì„ ë§Œë“¤ê³  OPENAI_API_KEY=sk-xxx í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ì„¸ìš”.\n"
                "2. ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”: export OPENAI_API_KEY=sk-xxx"
            )
        
        self.client = OpenAI(api_key=api_key)

    def get_news_url(self, keyword: str, max_articles: int) -> List[str]:
        encoded = urllib.parse.quote(keyword)
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ì—ì„œ ê¸°ì‚¬ URL ìˆ˜ì§‘"""
        url = f"https://search.naver.com/search.naver?ssc=tab.news.all&where=news&sm=tab_jum&query={encoded}"

        resp = requests.get(url, headers=self.headers, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        
        news_urls = []
        
        # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ìš”ì†Œ
        for span in soup.select(
            "span.sds-comps-text.sds-comps-text-ellipsis.sds-comps-text-ellipsis-1"
        ):
            text = span.get_text(strip=True)
            if text != "ë„¤ì´ë²„ë‰´ìŠ¤":
                continue

            a_tag = span.find_parent("a")
            if not a_tag:
                continue

            href = a_tag.get("href")

            if href and "news.naver.com" in href:
                if href not in news_urls:
                    news_urls.append(href)

            if len(news_urls)  >= max_articles:
                break

        return news_urls

    def extract_news_article(self, url: str) -> Dict[str, str]:
        """BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            resp = requests.get(url, headers=self.headers, timeout=10)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")

            # ì œëª© ì¶”ì¶œ
            title_html = soup.select_one("#title_area")
            title = title_html.get_text(strip=True) if title_html else None

            # ë³¸ë¬¸ ì¶”ì¶œ
            article = soup.select_one("#dic_area")
            text = None

            if article:
                for tag in article.find_all(['img', 'script', 'style', 'iframe']):
                    tag.decompose()
                        
                text = article.get_text(separator=' ', strip=True)
                text = ' '.join(text.split())

            if not title and not text:
                return {
                    'url': url,
                    'title': None,
                    'text': None,
                    'success': False,
                    'error': 'ì œëª©ê³¼ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                }
                    
            return {
                'url': url,
                'title': title,
                'text': text,
                'success': True,
            }
        
        except requests.RequestException as e:
            return {
                'url': url,
                'title': None,
                'text': None,
                'success': False,
                'error': f'ìš”ì²­ ì‹¤íŒ¨: {str(e)}',
            }
        except Exception as e:
            return {
                'url': url,
                'title': None,
                'text': None,
                'success': False,
                'error': f'íŒŒì‹± ì‹¤íŒ¨: {str(e)}',
            }
        
    def crawl_news(self, keyword: str, max_articles: int = 5) -> List[Dict[str, str]]:
        """
        í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ â†’ ëª¨ë“  ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
        
        Returns:
            ê¸°ì‚¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ì œëª©, ë³¸ë¬¸, URL ë“±)
        """
        urls = self.get_news_url(keyword, max_articles)

        articles = []
        for url in urls:
            result = self.extract_news_article(url)
            articles.append(result)

        return articles

    def prepare_articles_for_embedding(self, articles: List[Dict]) -> tuple[List[Dict], List[str]]:
        """
        í¬ë¡¤ë§ ê²°ê³¼ì—ì„œ ì„±ê³µí•œ ê¸°ì‚¬ë§Œ ì¶”ì¶œí•˜ê³  ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì¤€ë¹„

        Args:
            articles: í¬ë¡¤ë§ ê²°ê³¼ (ê¸°ì‚¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸)

        Returns:
            (valid_articles, texts_for_embedding)
            - valid_articles: ì„±ê³µí•œ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            - texts_for_embedding: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        valid_articles = []
        texts_for_embedding = []

        for article in articles:
            if not article['success']:
                continue
            if not article.get('text'):
                continue

            valid_articles.append(article)

            title = article.get('title') or ''
            text = article.get('text') or ''

            combined = f"[ì œëª©] {title}\n\n[ë³¸ë¬¸] {text[:1500]}"
            texts_for_embedding.append(combined)
        
        return valid_articles, texts_for_embedding
        
    
    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        ê¸°ì‚¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

        Args:
            texts: ë³€í™˜í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            numpy ë°°ì—´ (texts ê°œìˆ˜ x 1536 ì°¨ì›)
        """
        if not self.client:
            raise ValueError("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        texts = [t if t else " " for t in texts]

        response = self.client.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )

        embeddings = [item.embedding for item in response.data]
        
        return np.array(embeddings)
    
    def cluster_articles(
        self, embeddings: np.ndarray, articles: List[Dict],
        n_clusters: int = 3) -> List[Dict]:
        """
        ì„ë² ë”© ë²¡í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì‚¬ë“¤ì„ í´ëŸ¬ìŠ¤í„°ë§

        Args:
            embeddings: ì„ë² ë”© ë²¡í„° ë°°ì—´ (ê¸°ì‚¬ ìˆ˜ x 1536)
            articles: ê¸°ì‚¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            n_clusters: í´ëŸ¬ìŠ¤í„° ê°œìˆ˜

        Returns:
            í´ëŸ¬ìŠ¤í„°ë³„ ê¸°ì‚¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            [
                {
                    'cluster_id': 0,
                    'articles' : [ê¸°ì‚¬1, ê¸°ì‚¬2, ...],
                    'representative': ëŒ€í‘œ ê¸°ì‚¬,
                },
                ...
            ]
        """
        n_clusters = min(n_clusters, len(articles))
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(embeddings)
        
        clusters = []
        for cluster_id in range(n_clusters):
            indices = np.where(labels == cluster_id)[0]
            
            if len(indices) == 0:
                continue
            
            cluster_articles = [articles[i] for i in indices]
            cluster_embeddings = embeddings[indices]
            
            centroid = kmeans.cluster_centers_[cluster_id]
            distances = cosine_distances([centroid], cluster_embeddings)[0]
            representative_idx = np.argmin(distances)
            representative = cluster_articles[representative_idx]
            
            clusters.append({
                'cluster_id': cluster_id,
                'articles': cluster_articles,
                'representative': representative,
                'size': len(cluster_articles)
            })
            
        return clusters
    
    def summarize_cluster(self, cluster: Dict) -> Dict:
        """
        í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ê¸°ì‚¬ ìš”ì•½ + ê´€ë ¨ ê¸°ì‚¬ ì œëª© ë¦¬ìŠ¤íŠ¸
        
        Returns:
            {
                'cluster_id': í´ëŸ¬ìŠ¤í„° ID,
                'size': ê¸°ì‚¬ ê°œìˆ˜,
                'summary': ëŒ€í‘œ ê¸°ì‚¬ ìš”ì•½,
                'representative_title': ëŒ€í‘œ ê¸°ì‚¬ ì œëª©,
                'related_titles': ê´€ë ¨ ê¸°ì‚¬ ì œëª© ë¦¬ìŠ¤íŠ¸
            }
        """
        representative = cluster['representative']
        text = representative.get('text', '')
        
        # ëŒ€í‘œ ê¸°ì‚¬ ìš”ì•½
        if text:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # ë³€ê²½
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 3ë¬¸ì¥ ì´ë‚´ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì£¼ì„¸ìš”. "
                            "í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
                        )
                    },
                    {
                        "role": "user",
                        "content": text
                    }
                ],
                max_tokens=300,      # ì›ë˜ëŒ€ë¡œ
                temperature=0.3      # ì›ë˜ëŒ€ë¡œ
            )
            summary = response.choices[0].message.content
        else:
            summary = "ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ê´€ë ¨ ê¸°ì‚¬ ì œëª© ë¦¬ìŠ¤íŠ¸ (ëŒ€í‘œ ê¸°ì‚¬ ì œì™¸)
        related_titles = [
            article['title'] 
            for article in cluster['articles'] 
            if article['title'] != representative['title']
        ]
        
        return {
            'cluster_id': cluster['cluster_id'],
            'size': cluster['size'],
            'summary': summary,
            'representative_title': representative['title'],
            'related_titles': related_titles
        }


    def summarize_all_clusters(self, clusters: List[Dict]) -> List[Dict]:
        """ëª¨ë“  í´ëŸ¬ìŠ¤í„° ìš”ì•½"""
        results = []
        for cluster in clusters:
            result = self.summarize_cluster(cluster)
            results.append(result)
        return results
    
    def run(self, keyword: str, max_articles: int = 20, n_clusters: int =3) -> List[Dict]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: í¬ë¡¤ë§ -> ì„ë² ë”© -> í´ëŸ¬ìŠ¤í„°ë§ -> ìš”ì•½

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_articles: ìµœëŒ€ í¬ë¡¤ë§ ê¸°ì‚¬ ìˆ˜
            n_clusters: í´ëŸ¬ìŠ¤í„° ê°œìˆ˜

        Returns:
            í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        
        articles = self.crawl_news(keyword, max_articles)
        valid_articles, texts = self.prepare_articles_for_embedding(articles)
        
        if len(valid_articles) == 0:
            print(f"âŒ '{keyword}'ì— ëŒ€í•œ ìœ íš¨í•œ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return [] 
        
        embeddings = self.get_embeddings(texts)
        clusters = self.cluster_articles(embeddings, valid_articles, n_clusters)
        results = self.summarize_all_clusters(clusters)
        
        return results

if __name__ == "__main__": 
    summarizer = NewsSummarizer()

    keyword = input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    results = summarizer.run(keyword, max_articles=20, n_clusters=3)
    
    for result in results:
        print(f"\n{'='*60}")
        print(f"[ê·¸ë£¹ {result['cluster_id'] + 1}] - {result['size']}ê°œ ê¸°ì‚¬")
        print(f"{'='*60}")
        print(f"\nğŸ“° ëŒ€í‘œ ê¸°ì‚¬: {result['representative_title']}")
        print(f"\nğŸ“ ìš”ì•½:\n{result['summary']}")
        
        if result['related_titles']:
            print(f"\nğŸ”— ê´€ë ¨ ê¸°ì‚¬:")
            for title in result['related_titles']:
                print(f"   - {title}")
    
    
   
    